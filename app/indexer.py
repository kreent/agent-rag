"""
Script para indexar documentos de la carpeta /files.
Ejecutar una vez inicialmente y luego cuando haya cambios.
"""
import os
import hashlib
import json
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from unstructured.partition.auto import partition
from tqdm import tqdm

from app.vector_store import VectorStore

# Configuraci√≥n
FILES_PATH = os.getenv("FILES_PATH", "/files")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1000"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "4"))
STATE_FILE = "./data/index_state.json"

# Extensiones soportadas
SUPPORTED_EXTENSIONS = {
    ".pdf", ".docx", ".doc", ".xlsx", ".xls", 
    ".pptx", ".ppt", ".txt", ".md", ".csv",
    ".html", ".htm", ".rtf", ".odt", ".epub"
}


def get_file_hash(filepath: str) -> str:
    """Genera hash MD5 del archivo para detectar cambios."""
    hasher = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def load_state() -> dict:
    """Carga el estado de indexaci√≥n previo."""
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, "r") as f:
            return json.load(f)
    return {"files": {}, "last_run": None}


def save_state(state: dict):
    """Guarda el estado de indexaci√≥n."""
    os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
    state["last_run"] = datetime.now().isoformat()
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)


def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> list[str]:
    """Divide texto en chunks con overlap."""
    if len(text) <= chunk_size:
        return [text] if text.strip() else []
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Intentar cortar en un punto natural (espacio, punto, salto de l√≠nea)
        if end < len(text):
            for sep in ["\n\n", "\n", ". ", " "]:
                last_sep = chunk.rfind(sep)
                if last_sep > chunk_size // 2:
                    chunk = chunk[:last_sep + len(sep)]
                    end = start + len(chunk)
                    break
        
        if chunk.strip():
            chunks.append(chunk.strip())
        
        start = end - overlap
    
    return chunks


def extract_text_from_file(filepath: str) -> str:
    """Extrae texto de cualquier tipo de archivo soportado."""
    try:
        elements = partition(
            filename=filepath,
            strategy="auto",  # Usa OCR si es necesario
            include_page_breaks=True,
            languages=["spa"],  # Espa√±ol
        )
        
        # Unir todos los elementos
        text_parts = []
        for element in elements:
            text = str(element).strip()
            if text:
                text_parts.append(text)
        
        return "\n\n".join(text_parts)
    
    except Exception as e:
        print(f"‚ö†Ô∏è Error procesando {filepath}: {str(e)}")
        return ""


def process_file(filepath: str, file_id_prefix: str) -> list[dict]:
    """Procesa un archivo y retorna lista de chunks como documentos."""
    text = extract_text_from_file(filepath)
    
    if not text.strip():
        return []
    
    chunks = chunk_text(text)
    
    documents = []
    for i, chunk in enumerate(chunks):
        doc_id = f"{file_id_prefix}_chunk_{i}"
        documents.append({
            "id": doc_id,
            "content": chunk,
            "metadata": {
                "source": filepath,
                "chunk_index": i,
                "total_chunks": len(chunks)
            }
        })
    
    return documents


def get_files_to_process(state: dict) -> tuple[list, list, list]:
    """
    Determina qu√© archivos procesar.
    
    Returns:
        (nuevos, modificados, eliminados)
    """
    current_files = {}
    
    # Escanear directorio
    for root, _, files in os.walk(FILES_PATH):
        for filename in files:
            ext = Path(filename).suffix.lower()
            if ext not in SUPPORTED_EXTENSIONS:
                continue
            
            filepath = os.path.join(root, filename)
            try:
                file_hash = get_file_hash(filepath)
                current_files[filepath] = file_hash
            except Exception as e:
                print(f"‚ö†Ô∏è No se pudo leer {filepath}: {e}")
    
    previous_files = state.get("files", {})
    
    # Clasificar archivos
    nuevos = [f for f in current_files if f not in previous_files]
    modificados = [
        f for f in current_files 
        if f in previous_files and current_files[f] != previous_files[f]
    ]
    eliminados = [f for f in previous_files if f not in current_files]
    
    return nuevos, modificados, eliminados, current_files


def indexar(full_reindex: bool = False):
    """
    Indexa documentos de forma incremental o completa.
    
    Args:
        full_reindex: Si True, reindexar todo desde cero
    """
    print("=" * 60)
    print("üîç INDEXACI√ìN DE DOCUMENTOS")
    print("=" * 60)
    
    vector_store = VectorStore()
    
    if full_reindex:
        print("üóëÔ∏è Limpiando √≠ndice existente...")
        vector_store.limpiar()
        state = {"files": {}}
    else:
        state = load_state()
    
    # Determinar qu√© procesar
    nuevos, modificados, eliminados, current_files = get_files_to_process(state)
    
    print(f"\nüìä Archivos encontrados:")
    print(f"   - Nuevos: {len(nuevos)}")
    print(f"   - Modificados: {len(modificados)}")
    print(f"   - Eliminados: {len(eliminados)}")
    
    # Eliminar documentos de archivos eliminados/modificados
    for filepath in eliminados + modificados:
        file_id = hashlib.md5(filepath.encode()).hexdigest()[:12]
        vector_store.eliminar_por_source(filepath)
    
    # Procesar archivos nuevos y modificados
    archivos_a_procesar = nuevos + modificados
    
    if not archivos_a_procesar:
        print("\n‚úÖ No hay archivos nuevos que procesar")
        save_state({"files": current_files})
        return
    
    print(f"\nüìÑ Procesando {len(archivos_a_procesar)} archivos...")
    
    total_docs = 0
    errores = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {}
        
        for filepath in archivos_a_procesar:
            file_id = hashlib.md5(filepath.encode()).hexdigest()[:12]
            future = executor.submit(process_file, filepath, file_id)
            futures[future] = filepath
        
        with tqdm(total=len(futures), desc="Indexando") as pbar:
            for future in as_completed(futures):
                filepath = futures[future]
                try:
                    documentos = future.result()
                    if documentos:
                        vector_store.agregar_documentos(documentos)
                        total_docs += len(documentos)
                except Exception as e:
                    errores.append((filepath, str(e)))
                
                pbar.update(1)
    
    # Guardar estado
    save_state({"files": current_files})

    # Reconstruir √≠ndice BM25 para b√∫squeda h√≠brida
    if total_docs > 0:
        try:
            from app.search_pipeline import HybridSearchPipeline
            pipeline = HybridSearchPipeline()
            pipeline.build_bm25()
            print("‚úì √çndice BM25 reconstruido")
        except Exception as e:
            print(f"‚ö†Ô∏è No se pudo reconstruir BM25: {e}")
    
    # Resumen
    print("\n" + "=" * 60)
    print("üìä RESUMEN DE INDEXACI√ìN")
    print("=" * 60)
    print(f"‚úÖ Documentos indexados: {total_docs} chunks")
    print(f"üìÅ Total en √≠ndice: {vector_store.stats()['total_documents']} chunks")
    
    if errores:
        print(f"\n‚ö†Ô∏è Errores ({len(errores)}):")
        for filepath, error in errores[:10]:  # Mostrar m√°ximo 10
            print(f"   - {Path(filepath).name}: {error}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Indexar documentos")
    parser.add_argument("--full", action="store_true", help="Reindexar todo desde cero")
    args = parser.parse_args()
    
    indexar(full_reindex=args.full)
